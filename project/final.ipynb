{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 租房数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "本项目对各城市租房的数据进行分析，主要包括以下三个部分：\n",
    "\n",
    "- 数据获取\n",
    "- 数据分析\n",
    "- 数据可视化\n",
    "\n",
    "数据获取部分爬取了北京、上海、广州、深圳、南昌这 5 个城市全部租房数据，包含月租金、户型、朝向、面积、板块等信息。\n",
    "\n",
    "数据分析部分分析了各城市总体租房情况租金的均价、最高价、最低价、中位数等信息，对比了各城市人均 GDP 、平均工资等信息与租房情况的关系。\n",
    "\n",
    "数据可视化部分对以上分析进行了数量、比例、分布关系等方面的图形化直观展示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据获取\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据来源于链家官网的租房数据。\n",
    "\n",
    "首先导入爬取数据需要的库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # 用于获取网页内容\n",
    "from bs4 import BeautifulSoup as bs # 用于解析网页内容\n",
    "import time # 用于延时，避免爬取过快被封IP\n",
    "import csv # 用于将数据写入 csv 文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为方便开发时对代码进行调试，设置调试模式开关。调试时打开，最后正确代码执行时关闭，避免输出过多影响报告内容。\n",
    "\n",
    "伪造请求头用于简单的反爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调试模式开关\n",
    "debug = True\n",
    "\n",
    "# 伪造请求头\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) \\\n",
    "                   AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "                   Chrome/119.0.0.0 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为提高代码的复用性和可扩展性，以下是爬虫程序所用到的函数，最后只需要调用 `scrape()` 即可执行完整的爬虫程序："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHTMLText(url: str) -> str:\n",
    "    \"\"\"获取单个网页的 HTML 字符串内容\n",
    "\n",
    "    Args:\n",
    "        url (str): 网页的 URL\n",
    "\n",
    "    Returns:\n",
    "        str: 网页的 HTML 字符串内容\n",
    "    \"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30, headers=headers)\n",
    "        r.raise_for_status() # 如果状态不是200，引发HTTPError异常\n",
    "        r.encoding = r.apparent_encoding # 使得解码正确\n",
    "        return r.text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def getRentList(rentURL: str) -> list:\n",
    "    \"\"\"获取单个网页的租房信息列表\n",
    "\n",
    "    Args:\n",
    "        rentURL (str): 网页的 URL\n",
    "\n",
    "    Raises:\n",
    "        Exception: 租房信息格式不匹配\n",
    "\n",
    "    Returns:\n",
    "        list: 某单个网页租房信息列表，包括标题、行政区、板块、小区、面积、朝向、户型、租金\n",
    "    \"\"\"\n",
    "    rentList = []\n",
    "\n",
    "    # 获取网页内容\n",
    "    html = getHTMLText(rentURL)\n",
    "    if html == None:\n",
    "        print(f'{rentURL}访问异常') if debug == True else None\n",
    "        return rentList\n",
    "    \n",
    "    # 解析网页内容\n",
    "    soup = bs(html, 'html.parser')\n",
    "    house_list = soup.find_all('div', attrs={'class': 'content__list--item--main'})\n",
    "\n",
    "    for house in house_list:\n",
    "        # 租房信息的标题\n",
    "        p = house.find('p', attrs={'class': 'content__list--item--title'})\n",
    "        if p == None:\n",
    "            p = house.find('p', attrs={'class': 'content__list--item--title twoline'})\n",
    "        title = p.find('a').text.strip()\n",
    "        # 租房信息的描述\n",
    "        description = house.find('p', attrs={'class': 'content__list--item--des'})\n",
    "        # 位置\n",
    "        location = description.find_all('a', attrs={'target': '_blank'})\n",
    "        # location可能没有\n",
    "        if len(location) == 0:\n",
    "            print(f'[{title}] 缺少位置信息，已跳过...') if debug == True else None\n",
    "            continue\n",
    "        # 行政区\n",
    "        district = location[0].text.strip() # district\n",
    "        # 板块\n",
    "        block = location[1].text.strip() # block\n",
    "        # 小区\n",
    "        community = location[2].text.strip() # community\n",
    "        try:\n",
    "            # 把面积、朝向、户型、租金分开\n",
    "            des = description.get_text(strip=True).split('/')\n",
    "            # 有时候有广告，第一个元素是 \"精选\"，手动去除\n",
    "            if des[0] == '精选':\n",
    "                des.pop(0)\n",
    "            # 面积如果是范围取平均值\n",
    "            area_range = des[1].replace('㎡', '').strip().split('-')\n",
    "            area = float(area_range[0]) if len(area_range) == 1\\\n",
    "                                        else (float(area_range[0]) + float(area_range[1])) / 2\n",
    "            # 朝向\n",
    "            direction = des[2].replace(' ', '')\n",
    "            check = ['东', '南', '西', '北']\n",
    "            if not any([ch in direction for ch in check]):\n",
    "                print(f'[{title}] 朝向不是东南西北中的一个') if debug == True else None\n",
    "                raise Exception # 朝向不是东南西中的一个，格式不匹配\n",
    "            # 户型\n",
    "            type = des[3].strip()\n",
    "        except:\n",
    "            print(f'[{title}] 租房信息格式不匹配，已跳过...') if debug == True else None\n",
    "            continue\n",
    "        # 租金如果是范围取平均值\n",
    "        price_range = house.find('span', attrs={'class': 'content__list--item-price'})\\\n",
    "                    .find('em').text.strip()\\\n",
    "                    .split('-')\n",
    "        price = float(price_range[0]) if len(price_range) == 1\\\n",
    "                                    else (float(price_range[0]) + float(price_range[1])) / 2\n",
    "        # 将信息添加到列表中\n",
    "        rentList.append([title, district, block, community, area, direction, type, price])\n",
    "\n",
    "    time.sleep(1) # 延时 1s\n",
    "    return rentList\n",
    "\n",
    "def getCityRent(city: str, page: int) -> list:\n",
    "    \"\"\"获取一个城市若干页的租房信息列表\n",
    "\n",
    "    Args:\n",
    "        city (str): 城市的缩写，与链家网站的 URL 一致\n",
    "        page (int): 要爬取的页数\n",
    "\n",
    "    Returns:\n",
    "        list: 某城市若干页的租房信息列表，包括标题、行政区、板块、小区、面积、朝向、户型、租金\n",
    "    \"\"\"\n",
    "    rentList = []\n",
    "    for i in range(1, page + 1):\n",
    "        print(f'正在爬取第 {i} 页...') if debug == True else None\n",
    "        rentURL = f'https://{city}.lianjia.com/zufang/pg{i}/'\n",
    "        rentList += getRentList(rentURL)\n",
    "        time.sleep(1) # 延时 1s\n",
    "    return rentList\n",
    "\n",
    "def saveRentList(rentList: list, city: str):\n",
    "    \"\"\"将租房信息列表保存到 csv 文件中\n",
    "\n",
    "    Args:\n",
    "        rentList (list): 租房信息列表\n",
    "        city (str): 城市的缩写，与链家网站的 URL 一致\n",
    "    \"\"\"\n",
    "    print(f'正在保存 {city} 租房信息...')\n",
    "    with open(f'{city}_rent.csv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['标题', '行政区', '板块', '小区', '面积', '朝向', '户型', '租金'])\n",
    "        writer.writerows(rentList)\n",
    "    print(f'{city} 租房信息保存成功!')\n",
    "\n",
    "def scrape():\n",
    "    \"\"\"爬取租房信息，爬虫程序入口\n",
    "    \"\"\"\n",
    "    cites = ['bj', 'sh', 'gz', 'sz', 'nc'] # 爬取的城市\n",
    "    page = 10 # 爬取的页数\n",
    "    for city in cites:\n",
    "        print(f'正在爬取 {city} 租房信息...') if debug == True else None\n",
    "        rentList = getCityRent(city, page)\n",
    "        saveRentList(rentList, city)\n",
    "        print(f'{city} 爬取完成!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行爬虫进行数据获取："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape() # 启动爬虫程序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所爬取的五个城市信息存储在以下五个 csv 文件中：\n",
    "\n",
    "- 北京：`bj_rent.csv`\n",
    "- 上海：`sh_rent.csv`\n",
    "- 广州：`gz_rent.csv`\n",
    "- 深圳：`sz_rent.csv`\n",
    "- 南昌：`nc_rent.csv`\n",
    "\n",
    "表头格式为：\n",
    "\n",
    "```\n",
    "标题,行政区,板块,小区,面积,朝向,户型,租金\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据可视化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
